---
layout: single
permalink: /research/
title: "Research"
header:
  overlay_color: "#5e616c"
  overlay_image: Image_network.jpg
  # caption: "[Image credit: **NASA/ESA**](https://www.spacetelescope.org/images/heic0515a/)"
excerpt: 
author: PierBeneventano
author_profile: true
comments: false
---

I have been involved in the following research projects. 




<hr>

<div class="col-lg-4 text-center">
  <a href="https://www.linkedin.com/in/pierbeneventano/"><span class="social-icon fa fa-linkedin"></span></a> &nbsp;  &nbsp;   <a href="https://join.skype.com/invite/kobWyHxDkzse"><span  class="social-icon fa fa-skype"></span></a> &nbsp;  &nbsp;  <a href="https://pierbeneventano.github.io/CV/CV_Beneventano.pdf" class="links"> CV </a> &nbsp;  &nbsp; <a href="https://scholar.google.com/citations?user=spL439oAAAAJ&hl=en"><span class="ai ai-fw ai-google-scholar-square"></span></a> &nbsp;  &nbsp; <a href="mailto:pierb@princeton.edu"><span class="social-icon fa fa-envelope"></span></a>
</div>

<hr>

## Neural network approximation theory
The goal is to understand which is the family of functions which are approximable without curse of dimensionality.
Coauthors: <a href="https://scholar.google.de/citations?user=fymm-XQAAAAJ&hl=en" class="links">Prof. Arnulf Jentzen</a>, <a href="https://people.math.ethz.ch/~patrickc/" class="links">Prof. Patrick Cheridito</a>, <a href="https://scholar.google.com/citations?user=Dc8yKjUAAAAJ&hl=en" class="links">Philippe von Wurstemberger</a>, Robin Graeber.
<details>
  <summary><a href="https://arxiv.org/abs/2012.04326" class="links">High-dimensional approximation spaces of artificial neural networks and applications to partial differential equations.</a></summary>

  This work has been my "semester paper" during my degree at ETH.

  In this work we develop a new machinery to study the capacity of neural networks to approximate high-dimensional functions without suffering from the curse of dimensionality. We then use our machinery to prove that the solutions of certain easy PDEs are arbitrarily approximable without the curse of dimensionality.
</details>

<details>
  <summary>Deep neural network approximations for high-dimensional functions.</summary>

  Soon on arXiv, this work has been my thesis during my degree at ETH.

  On the line of the previous work, but more general, we provide a suitable large class of functions that can be approximated by DNNs without the curse of dimensionality. The main contributions of this thesis are the following: (a) the discovery of new cost bounds in the approximation of the product of d âˆˆ N real numbers and of representation the maximum of d real numbers, (b) the introduction of some DNN approximation spaces of functions and the proof that they are closed for some operations, and, as a consequence, (c) the proof that DNNs overcome the curse of dimensionality in the approximation on any compact set of products, maxima or the combination of both applied to low dimensional locally Lipschitz continuous functions.
</details>

## Errors in modelling - Hubris of the scientist
