---
layout: single
permalink: /research/
title: "Research"
header:
  overlay_color: "#5e616c"
  overlay_image: Image_network.jpg
  # caption: "[Image credit: **NASA/ESA**](https://www.spacetelescope.org/images/heic0515a/)"
excerpt: 
author: PierBeneventano
author_profile: true
comments: false
---

<!-- # Current research  -->




<hr>

<div class="col-lg-4 text-center">
  <div class="profile">
      <!-- <img src="./images/face_016.jpg" width="120" height="160" > -->
      <!-- <h2>Contacts</h2> -->
      <!-- <img height="20" width="20" src="./assets/icons/mail.svg" /> <a class="contact-link"href=""> pierb@princeton.edu</a>
      <img height="20" width="20" src="./assets/icons/graduation.svg" /> <a class="contact-link"href="https://scholar.google.com/citations?user=spL439oAAAAJ&hl=en"> Google Scholar</a>
      <img height="20" width="20" src="./assets/icons/user.svg" /> <a class="contact-link"href="https://pierbeneventano.github.io/CV/CV_Beneventano.pdf"> Curriculum vitae</a> -->
      <a href="https://www.linkedin.com/in/pierbeneventano/"><span class="social-icon fa fa-linkedin"></span></a> &nbsp;  &nbsp;  
      <a href="https://join.skype.com/invite/kobWyHxDkzse"><span  class="social-icon fa fa-skype"></span></a> &nbsp;  &nbsp; 
      <a href="https://www.instagram.com/prbn96/?hl=en"><span  class="social-icon fa fa-instagram"></span></a> &nbsp;  &nbsp; 
      <a href="https://github.com/PierBeneventano"><span  class="social-icon fa fa-github"></span></a> &nbsp;  &nbsp; 
      <a href="https://pierbeneventano.github.io/CV/CV_Beneventano.pdf" class="links"> CV </a> &nbsp;  &nbsp; 
      <a href="https://scholar.google.com/citations?user=spL439oAAAAJ&hl=en"><span class="ai ai-fw ai-google-scholar-square"></span></a> &nbsp;  &nbsp; 
      <a href="mailto:pierb@princeton.edu"><span class="social-icon fa fa-envelope"></span></a>
  </div>
</div>

<hr>


<!-- # Past research -->

## Neural network approximation theory

The goal is to understand which is the family of functions which are approximable without curse of dimensionality.

Coauthors: <a href="https://scholar.google.de/citations?user=fymm-XQAAAAJ&hl=en" class="links">Prof. Arnulf Jentzen</a>, <a href="https://people.math.ethz.ch/~patrickc/" class="links">Prof. Patrick Cheridito</a>, <a href="https://scholar.google.com/citations?user=Dc8yKjUAAAAJ&hl=en" class="links">Philippe von Wurstemberger</a>, Robin Graeber.

<details style = "font-size:0.8em;">
  <summary style = "font-size:0.9em;"><a href="https://arxiv.org/abs/2012.04326" class="links">High-dimensional approximation spaces of artificial neural networks and applications to partial differential equations.</a></summary>

  This work has been my "semester paper" during my degree at ETH.

  In this work we develop a new machinery to study the capacity of neural networks to approximate high-dimensional functions without suffering from the curse of dimensionality. We then use our machinery to prove that the solutions of certain easy PDEs are arbitrarily approximable without the curse of dimensionality.
</details>

<details style = "font-size:0.8em;">
  <summary style = "font-size:0.9em;">Deep neural network approximations for high-dimensional functions. (to appear soon)</summary>

  Soon on arXiv, this work has been my thesis during my degree at ETH.

  On the line of the previous work, but more general, we provide a suitable large class of functions that can be approximated by DNNs without the curse of dimensionality. The main contributions of this thesis are the following: (a) the discovery of new cost bounds in the approximation of the product of d âˆˆ N real numbers and of representation the maximum of d real numbers, (b) the introduction of some DNN approximation spaces of functions and the proof that they are closed for some operations, and, as a consequence, (c) the proof that DNNs overcome the curse of dimensionality in the approximation on any compact set of products, maxima or the combination of both applied to low dimensional locally Lipschitz continuous functions.
</details>


## Responsible mathematical modeling
We are writing a short comment on the on the limits of mathematical modeling, statistics, and machine learning. The focus is on the O'neil conjecture and the fact that it happens that modeler do not account for it, in a sort of <em>modeler hubris</em>.

Coauthors: <a href="https://scholar.google.com/citations?user=vqhLsGkAAAAJ&hl=en" class="links">Andrea Saltelli</a>, <a href="https://scholar.google.com/citations?user=3h35F_4AAAAJ&hl=en" class="links">Tommaso Potaluri</a>, <a href="https://scholar.google.com/citations?user=lGgh0DoAAAAJ&hl=en" class="links">Arnald Puy</a>, and <a href="https://scholar.google.com/citations?user=NyVVh7kAAAAJ&hl=en" class="links">Samuele Lo Piano</a>.
